# Clustering

## Introduction

What is clustering? Simply, grouping objects together that have a meaningful relationship

**Binary relationship** relationships between two objects.

Examples- reflexivity, symmetry, transitive

## Relationships

### Binary
A binary relationship that satisfies all three of these properties is an **equivalence relation.**

Sequence similarity: is not an equivalence relationship (reflexive, symmetric, but not transitive)

### Distance
**Distance relation:** function that considers the distance between two objects in a geometric sense

Must satisfy:
  1. D(a, a) = 0
  2. D(a, b) >= 0
  3. D(a, b) = D(b, a)
  4. D(a, b) <= D(a, c) + D(c, b)

examples- euclidian distance, city block

### Similarity

A **similarity function** requires that
  1. 0 <= S(a, b) <= 1
  2. S(a, a) = 1
  3. S(a, b) = S(b, a)

Similarity relationship examples- % identity of strings, Tanimoto similarity of bitstrings

### Missing values
if we don't have complete data for similarity relationships. Can remove the objects, replace with
an average, use prior knowledge and replace with most likely value

### Normalizing values
Scaling so that all values contribute equally -- **normalizing**

Ex. divide by the mean absolute deviation

```
  Change each xi to zi, where
  zi = xi - m / S
  S = 1/n {|x0-m| + |x1-m| + … + |xn-m|}
  m = 1/n {x0 + x1 + … + xn}
```

## Clustering algorithms

2 types: partitioning and hierarchical

![cluster.png](https://github.com/christacaggiano/algorithms/blob/master/images/cluster.png)

## Partitioning algorithms

1. choose a representative set of objects
2. assign each remaining object to its nearest representative

### K-means

Input: a set of objects X, and an integer K
Output: A partition S of the objects that minimizes the sum of squared distance to the center of the cluster

1. Arbitrarily choose from the set of objects K initial cluster centers.
2. Assign each of the objects in X to the cluster
3. Recalculate the Step 3: cluster centers.
4. If the clusters have not changed (or a given number of iterations has been reached), terminate, else go to step 2.

K-means: O(n<sup>2</sup>) - the worst case needs to calculate the distance between all n objects

### Covering algorithms

Also based on choosing a representative set of objects, but don't require a predefined number of clusters.
Instead, use **distance thresholds** for objects within a cluster


## Hierarchical algorithms

2 types- divisive or agglomerative

### Agglomerative:

1. Put each object in its own cluster.
2. Choose (via a linkage criterion), two clusters and merge them.
3. If there is only one cluster left, stop. Else, go to step 2.

Types of linkage: single linkage, complete linkage, average linkage, centroid linkage

Most hierarchical algorithms are agglomerative because of run time-- O(n<sup>2</sup>) versus
O(2<sup>n</sup>) for divisive algorithms

### Divisive algorithms

Ex: **Macnaughton-Smith Method**

1. Find the object with the highest average distance from all of the others. This object forms a splinter set.
2. For each object left in the original set, calculate the difference between the average distance from objects in the set and the average distance from objects in the splinter set.
3. If all the differences are negative, stop. Else, move the object with the most positive difference to the splinter set and go to Step 2.

This algorithm is only O(n<sup>2</sup>)

**Fuzzy clustering** is another iteration on divisive algorithms - each object is assigned a vector of weights
representing membership in a cluster.

## Choosing and evaluating an algorithm

Considerations for choosing may include:
  * Total number of objects
  * Number of likely clusters
  * Shape of clusters

Evaluating criterion may include:
  * Sum of distances method- looks at the sum of distance between objects within a cluster or to a representative object.
  The lower the sum, the "better" the clustering
  * Purity - each cluster is assigned a class based  on the object that appears most frequently in the cluster
  * Rand index- Similar to the Purity measure, except now we can penalize incorrect assignments.
  * F-measure: does not treat false positives and false negatives equally. Calculates a relationship between precision and recall
  and then measure is tuned to favor one

Clustering in bioinformatics
| Objects | Variables | Algorithm |
|---------|-----------|-----------|
| genes | sequence, organisms, promoters | agglomerative|
| proteins | sequence, structure, function | partitioning, hierarchical |
| microarray | fluorescence, time, conditions| agglomerative |
| small compounds | structure, bioactivity | partitioning, hierarchical |
| patients | treatment, outcome, location | partitioning, hierarchical | 
