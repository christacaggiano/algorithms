# Clustering

## introduction

What is clustering? Simply, grouping objects together that have a meaningful relationship

**Binary relationship** relationships between two objects.

Examples- reflexivity, symmetry, transitive

## Relationships

### binary
A binary relationship that satisfies all three of these properties is an **equivalence relation.**

Sequence similarity: is not an equivalence relationship (reflexive, symmetric, but not transitive)

### distance
**Distance relation:** function that considers the distance between two objects in a geometric sense

Must satisfy:
  1. D(a, a) = 0
  2. D(a, b) >= 0
  3. D(a, b) = D(b, a)
  4. D(a, b) <= D(a, c) + D(c, b)

examples- euclidian distance, city block

### similarity

A **similarity function** requires that
  1. 0 <= S(a, b) <= 1
  2. S(a, a) = 1
  3. S(a, b) = S(b, a)

Similarity relationship examples- % identity of strings, Tanimoto similarity of bitstrings

### missing values
if we don't have complete data for similarity relationships. Can remove the objects, replace with
an average, use prior knowledge and replace with most likely value

### Normalizing values
Scaling so that all values contribute equally -- **normalizing**

Ex. divide by the mean absolute deviation

`Change each xi to zi, where

  zi = xi - m / S

  S = 1/n {|x0-m| + |x1-m| + … + |xn-m|}

  m = 1/n {x0 + x1 + … + xn}
`

## Clustering algorithms

2 types: partitioning and

![cluster.png](https://github.com/christacaggiano/algorithms/blob/master/images/cluster.png)

### partitioning algorithms

1. choose a representative set of objects
2. assign each remaining object to its nearest representative

#### K-means

Input: a set of objects X, and an integer K
Output: A partition S of the objects that minimizes the sum of squared distance to the center of the cluster

1. Arbitrarily choose from the set of objects K initial cluster centers.
2. Assign each of the objects in X to the cluster
3. Recalculate the Step 3: cluster centers.
4. If the clusters have not changed (or a given number of iterations has been reached), terminate, else go to step 2.

K-means: O(n^2) - the worst case needs to calculate the distance between all n objects

#### Covering algorithms

Also based on choosing a representative set of objects, but don't require a predefined number of clusters.
Instead, use **distance thresholds** for objects within a cluster


### Hierarchical algorithms

2 types- divisive or agglomerative

#### Agglomerative:

1. Put each object in its own cluster.
2. Choose (via a linkage criterion), two clusters and merge them.
3. If there is only one cluster left, stop. Else, go to step 2.

Types of linkage: single linkage, complete linkage, average linkage, centroid linkage

Most hierarchical algorithms are agglomerative because of run time-- O(n^2) versus
O(2^n) for divisive algorithms 
