# Hidden Markov Model


## Markov Process  

A process in which the state at time t depends upon the state at times t-1, t-2,..., t-k

**Markov chain** first order Markov process

Given a sequence of symbols and a HMM :
1. what's the most probable sequence of transitions and emissions - *decoding*
2. How likely is this sequence given the HMM - *likelihood*
3. How should the transition and emission probabilities be updated? - *learning*

## Decoding

### The Viterbi algorithm

**Viterbi algorithm** For any state at time t, there is only one most likely path to that state.

Let X be the sequence of symbols X<sub>1</sub>, X<sub>2</sub>, ... X<sub>L</sub>

Let V<sub>k</sub>(i) = probability of the most probably path for sequence X<sub>1</sub>, X<sub>2</sub>, ... X<sub>L</sub>  that ends in state K

Algorithm:

**V<sub>j</sub>(i+1) = e<sub>j(X<sub>i+1</sub>)  max<sub>k in S </sub> {V<sub>K</sub>(i) a<sub>kj</sub>}**

where,

**e<sub>j</sub>(X<sub>i</sub>)** = probability of emitting symbol X<sub>i</sub> from state j

**a<sub>kj</sub>** = probability of a transition from state k to state j

**Final probability** of sequence given the most probable path

**P(X|p*) = max<sub>k in S </sub> {V<sub>K</sub>(L) a<sub>kend</sub>}**


**Decoding** assigns each observed symbol to a state

## Likelihood

Given the sequence of symbols and a HMM, how likely is this sequence given the model? You could find all combinations of states and emission to give our sequence of symbols and calculate the sum of their probabilities -- but an infinite number of states!

### Forward algorithm

Given the sequence of symbols X= {X<sub>1</sub>, X<sub>2</sub>, ... X<sub>L</sub>}

let f<sub>k</sub>(i) = the probability of having emitted the prefix {X<sub>1</sub>, X<sub>2</sub>, ... X<sub>L</sub>}  and reaching state k


At the end of the calculation:

**P(X) = Sum(f<sub>k</sub> (L) a<sub>kend</sub>)**

### Backward algorithm

Given the sequence of symbols X= {X<sub>1</sub>, X<sub>2</sub>, ... X<sub>L</sub>}

let b<sub>k</sub>(i) = the probability of having emitted the suffix {X<sub>i+1</sub>, X<sub>i+2</sub>, ... X<sub>L</sub>}  and reaching state k


At the end,

**P(X) = Sum( a<sub>start k</sub> e<sub>x<sub>1</sub></sub> b<sub>k</sub>(1))**

## Learning

How should the transition and emission probabilities be updated given new sequences of symbols?

Given n sequences {X(1), X(2), ..., X(n)} of lengths L(1), L(2), ..., L(n) which were generated from HMM  

M(Y, S, q).

Assign values to q that maximize the probabilities of our sequences given the model.


### Maximum likelihood estimators

Say we know the state sequences for each sequence of symbols, we can count

A<sub>kj</sub> = # of transitions from state j to state k

E<sub>k</sub>(b) = number of times symbol B was emitted from state k


#### Baum-Welch Algorithm


If we don't know state sequences we can use an iterative algorithm to get close to estimating

*see notes for this* 
