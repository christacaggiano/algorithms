# Optimization

## Introduction

**Optimization-** given an objective function, f, find the set of values, x,
that maximize or minimize the value of f(x)

Optimization in bioinformatics
* Sequence alignment
* Structure prediction/alignment
* Energy minimization
* Biosynthetic pathway analysis
* Pattern recognition


Two types: stochastic vs Non-stochastic

Non-stochastic:
* typically follow a single trajectory
* are more common for local optimization


Stochastic:

* Start at multiple points in the
function space
* Only need to know how to
evaluate the objective function
* Are by far the most common for
global optimization tasks

## Types of Common Optimization Algorithms 

### Branch and Bound

Objective function f(x) where X{X<sub>1</sub>, X<sub>2</sub>, ... X<sub>n</sub> } and X<sub>i</sub> must be discrete

Using a **bounding function**, determine if
the best the selection of X<sub>i</sub>  can yield is
worse than the worst any other selection
of X<sub>i</sub>  could yield. If so, prune that branch
of the tree.


### Gradient descent

Objective function: f(x) which is first order differentiable
Start with point X<sub>0</sub> and calculate f(X<sub>0</sub>)
Choose a new point X<sub>2</sub> = X<sub>0</sub> – β Δ
 f(X<sub>0</sub>)where β is the step size


#### Neural net example
 0. Start by giving all of the weights w<sub>ij</sub> random small values
 1. for each weight, w<sub>ij</sub>, set  Δ w<sub>ij</sub> = 0
 2. For each point in the training data (x, t)
  1. set the inputs to x
  2. compute the values of the outputs y
  3. for each weight w<sub>ij</sub>,  Δ w<sub>ij</sub> =  Δw<sub>ij</sub> +(t<sub>i</sub> - y<sub>i</sub>) y<sub>i</sub>
 3. For each weight set w<sub>ij</sub> = w<sub>ij</sub> + β Δ w<sub>ij</sub>
 1. If the sum of changes of the weights is below the convergence threshold, stop. Else, go to step 1.
