# Optimization

## Introduction

**Optimization-** given an objective function, f, find the set of values, x,
that maximize or minimize the value of f(x)

Optimization in bioinformatics
* Sequence alignment
* Structure prediction/alignment
* Energy minimization
* Biosynthetic pathway analysis
* Pattern recognition


Two types: **stochastic** vs **Non-stochastic**

Non-stochastic:
* typically follow a single trajectory
* are more common for local optimization


Stochastic:

* Start at multiple points in the
function space
* Only need to know how to
evaluate the objective function
* Are by far the most common for
global optimization tasks

## Types of Common Optimization Algorithms

### Branch and Bound

Objective function f(x) where X{X<sub>1</sub>, X<sub>2</sub>, ... X<sub>n</sub> } and X<sub>i</sub> must be discrete

Using a **bounding function**, determine if
the best the selection of X<sub>i</sub>  can yield is
worse than the worst any other selection
of X<sub>i</sub>  could yield. If so, prune that branch
of the tree.


### Gradient descent

Objective function: f(x) which is first order differentiable
Start with point X<sub>0</sub> and calculate f(X<sub>0</sub>)
Choose a new point X<sub>2</sub> = X<sub>0</sub> – β Δ
 f(X<sub>0</sub>)where β is the step size


#### Neural net example

Example of gradient descent

 0. Start by giving all of the weights w<sub>ij</sub> random small values
 1. for each weight, w<sub>ij</sub>, set  Δ w<sub>ij</sub> = 0
 2. For each point in the training data (x, t)
  1. set the inputs to x
  2. compute the values of the outputs y
  3. for each weight w<sub>ij</sub>,  Δ w<sub>ij</sub> =  Δw<sub>ij</sub> +(t<sub>i</sub> - y<sub>i</sub>) y<sub>i</sub>
 3. For each weight set w<sub>ij</sub> = w<sub>ij</sub> + β Δ w<sub>ij</sub>
 1. If the sum of changes of the weights is below the convergence threshold, stop. Else, go to step 1.

## Linear Programming

Maximize f(x) = c<sub>0</sub>x<sub>0</sub> + c<sub>1</sub>x<sub>1</sub> + ... + c<sub>n</sub>x<sub>n</sub>

Subject to the constraints:

a<sub>00</sub>x<sub>0</sub> + a<sub>01</sub>x<sub>1</sub> + … + a<sub>0n</sub>x<sub>n</sub> ≤ b<sub>0</sub> AND x<sub>0</sub> ≥0

a<sub>10</sub>x<sub>0</sub> + a<sub>11</sub>x<sub>1</sub> + … + a<sub>1n</sub>x<sub>n</sub> ≤ b<sub>1</sub> AND x<sub>1</sub> ≥0


a<sub>n0</sub>x<sub>0</sub> + a<sub>n1</sub>x<sub>1</sub> + … + a<sub>nn</sub>x<sub>n</sub> ≤ b<sub>n</sub>  AND x<sub>n</sub> ≥0


The most widely used algorithm to solve linear programming
problems is the **simplex method**

Move along the vertices of a
polytope (simplex) defined by
the linear constraints.

**Pros/cons**
* choice of the initial simplex vertices can have significant impact- often many iterations are done
* direct method -doesn't require initial differentiable function
* handles noisy functions well
* does not guarantee a global minimum


## Monte Carlo/Simulated Annealing

1. We start in state x<sub>0</sub> and calculate its energy, e<sub>0</sub>  (=f(x<sub>0</sub> ))
2. We propose a new state x<sub>1</sub> and calculate its energy, e<sub>1</sub>
3. We accept x<sub>1</sub> as our new state according to a
probability function, *P(e<sub>0</sub>,e<sub>1</sub>,T)*
4. If improvement is below threshold, or max number
of iterations is reached, stop. Else, go to (1).


## Genetic algorithms

The function we want to maximize is the evolutionary fitness of a population of potential solution

Individual = x<sup>i</sup>

Population = < x >

Fitness = *f*(< x > )

1. choose an initial population
2. determine the fitness of each individual
3. perform selection
4. while termination criteria not met
 - perform crossover
 - perform mutation
 - determine the fitness of each individual
 - perform selection
